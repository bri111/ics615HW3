TruffleHog Analysis Report for project2
============================================================
Total secrets detected: 6

Secret #1:
{
  "branch": "origin/main",
  "commit": "Add LIBERO sim eval script and instructions\n",
  "commitHash": "a793354ecb3361e2627e7e96e7efdb43c8353de6",
  "date": "2024-09-05 03:06:55",
  "diff": "@@ -1,101 +0,0 @@\n-\"\"\"Utils for evaluating policies in LIBERO simulation environments.\"\"\"\n-\n-import math\n-import os\n-\n-import imageio\n-import numpy as np\n-import tensorflow as tf\n-from libero.libero import get_libero_path\n-from libero.libero.envs import OffScreenRenderEnv\n-\n-from experiments.robot.robot_utils import (\n-    DATE,\n-    DATE_TIME,\n-)\n-\n-\n-def get_libero_env(task, model_family, resolution=256):\n-    \"\"\"Initializes and returns the LIBERO environment, along with the task description.\"\"\"\n-    task_description = task.language\n-    task_bddl_file = os.path.join(get_libero_path(\"bddl_files\"), task.problem_folder, task.bddl_file)\n-    env_args = {\"bddl_file_name\": task_bddl_file, \"camera_heights\": resolution, \"camera_widths\": resolution}\n-    env = OffScreenRenderEnv(**env_args)\n-    env.seed(0)  # IMPORTANT: seed seems to affect object positions even when using fixed initial state\n-    return env, task_description\n-\n-\n-def get_libero_dummy_action(model_family: str):\n-    \"\"\"Get dummy/no-op action, used to roll out the simulation while the robot does nothing.\"\"\"\n-    return [0, 0, 0, 0, 0, 0, -1]\n-\n-\n-def resize_image(img, resize_size):\n-    \"\"\"\n-    Takes numpy array corresponding to a single image and returns resized image as numpy array.\n-\n-    NOTE (Moo Jin): To make input images in distribution with respect to the inputs seen at training time, we follow\n-                    the same resizing scheme used in the Octo dataloader, which OpenVLA uses for training.\n-    \"\"\"\n-    assert isinstance(resize_size, tuple)\n-    # Resize to image size expected by model\n-    img = tf.image.encode_jpeg(img)  # Encode as JPEG, as done in RLDS dataset builder\n-    img = tf.io.decode_image(img, expand_animations=False, dtype=tf.uint8)  # Immediately decode back\n-    img = tf.image.resize(img, resize_size, method=\"lanczos3\", antialias=True)\n-    img = tf.cast(tf.clip_by_value(tf.round(img), 0, 255), tf.uint8)\n-    img = img.numpy()\n-    return img\n-\n-\n-def get_libero_image(obs, resize_size):\n-    \"\"\"Extracts image from observations and preprocesses it.\"\"\"\n-    assert isinstance(resize_size, int) or isinstance(resize_size, tuple)\n-    if isinstance(resize_size, int):\n-        resize_size = (resize_size, resize_size)\n-    img = obs[\"agentview_image\"]\n-    img = img[::-1, ::-1]  # IMPORTANT: rotate 180 degrees to match train preprocessing\n-    img = resize_image(img, resize_size)\n-    return img\n-\n-\n-def save_rollout_video(rollout_images, idx, success, task_description, log_file=None):\n-    \"\"\"Saves an MP4 replay of an episode.\"\"\"\n-    rollout_dir = f\"./rollouts/{DATE}\"\n-    os.makedirs(rollout_dir, exist_ok=True)\n-    processed_task_description = task_description.lower().replace(\" \", \"_\").replace(\"\\n\", \"_\").replace(\".\", \"_\")[:50]\n-    mp4_path = f\"{rollout_dir}/{DATE_TIME}--episode={idx}--success={success}--task={processed_task_description}.mp4\"\n-    video_writer = imageio.get_writer(mp4_path, fps=30)\n-    for img in rollout_images:\n-        video_writer.append_data(img)\n-    video_writer.close()\n-    print(f\"Saved rollout MP4 at path {mp4_path}\")\n-    if log_file is not None:\n-        log_file.write(f\"Saved rollout MP4 at path {mp4_path}\\n\")\n-    return mp4_path\n-\n-\n-def quat2axisangle(quat):\n-    \"\"\"\n-    Copied from robosuite: https://github.com/ARISE-Initiative/robosuite/blob/eafb81f54ffc104f905ee48a16bb15f059176ad3/robosuite/utils/transform_utils.py#L490C1-L512C55\n-\n-    Converts quaternion to axis-angle format.\n-    Returns a unit vector direction scaled by its angle in radians.\n-\n-    Args:\n-        quat (np.array): (x,y,z,w) vec4 float angles\n-\n-    Returns:\n-        np.array: (ax,ay,az) axis-angle exponential coordinates\n-    \"\"\"\n-    # clip quaternion\n-    if quat[3] > 1.0:\n-        quat[3] = 1.0\n-    elif quat[3] < -1.0:\n-        quat[3] = -1.0\n-\n-    den = np.sqrt(1.0 - quat[3] * quat[3])\n-    if math.isclose(den, 0.0):\n-        # This is (close to) a zero degree rotation, immediately return\n-        return np.zeros(3)\n-\n-    return (quat[:3] * 2.0 * math.acos(quat[3])) / den\n",
  "path": "experiments/robot/libero/libero_utils.py",
  "printDiff": "@@ -1,101 +0,0 @@\n-\"\"\"Utils for evaluating policies in LIBERO simulation environments.\"\"\"\n-\n-import math\n-import os\n-\n-import imageio\n-import numpy as np\n-import tensorflow as tf\n-from libero.libero import get_libero_path\n-from libero.libero.envs import OffScreenRenderEnv\n-\n-from experiments.robot.robot_utils import (\n-    DATE,\n-    DATE_TIME,\n-)\n-\n-\n-def get_libero_env(task, model_family, resolution=256):\n-    \"\"\"Initializes and returns the LIBERO environment, along with the task description.\"\"\"\n-    task_description = task.language\n-    task_bddl_file = os.path.join(get_libero_path(\"bddl_files\"), task.problem_folder, task.bddl_file)\n-    env_args = {\"bddl_file_name\": task_bddl_file, \"camera_heights\": resolution, \"camera_widths\": resolution}\n-    env = OffScreenRenderEnv(**env_args)\n-    env.seed(0)  # IMPORTANT: seed seems to affect object positions even when using fixed initial state\n-    return env, task_description\n-\n-\n-def get_libero_dummy_action(model_family: str):\n-    \"\"\"Get dummy/no-op action, used to roll out the simulation while the robot does nothing.\"\"\"\n-    return [0, 0, 0, 0, 0, 0, -1]\n-\n-\n-def resize_image(img, resize_size):\n-    \"\"\"\n-    Takes numpy array corresponding to a single image and returns resized image as numpy array.\n-\n-    NOTE (Moo Jin): To make input images in distribution with respect to the inputs seen at training time, we follow\n-                    the same resizing scheme used in the Octo dataloader, which OpenVLA uses for training.\n-    \"\"\"\n-    assert isinstance(resize_size, tuple)\n-    # Resize to image size expected by model\n-    img = tf.image.encode_jpeg(img)  # Encode as JPEG, as done in RLDS dataset builder\n-    img = tf.io.decode_image(img, expand_animations=False, dtype=tf.uint8)  # Immediately decode back\n-    img = tf.image.resize(img, resize_size, method=\"lanczos3\", antialias=True)\n-    img = tf.cast(tf.clip_by_value(tf.round(img), 0, 255), tf.uint8)\n-    img = img.numpy()\n-    return img\n-\n-\n-def get_libero_image(obs, resize_size):\n-    \"\"\"Extracts image from observations and preprocesses it.\"\"\"\n-    assert isinstance(resize_size, int) or isinstance(resize_size, tuple)\n-    if isinstance(resize_size, int):\n-        resize_size = (resize_size, resize_size)\n-    img = obs[\"agentview_image\"]\n-    img = img[::-1, ::-1]  # IMPORTANT: rotate 180 degrees to match train preprocessing\n-    img = resize_image(img, resize_size)\n-    return img\n-\n-\n-def save_rollout_video(rollout_images, idx, success, task_description, log_file=None):\n-    \"\"\"Saves an MP4 replay of an episode.\"\"\"\n-    rollout_dir = f\"./rollouts/{DATE}\"\n-    os.makedirs(rollout_dir, exist_ok=True)\n-    processed_task_description = task_description.lower().replace(\" \", \"_\").replace(\"\\n\", \"_\").replace(\".\", \"_\")[:50]\n-    mp4_path = f\"{rollout_dir}/{DATE_TIME}--episode={idx}--success={success}--task={processed_task_description}.mp4\"\n-    video_writer = imageio.get_writer(mp4_path, fps=30)\n-    for img in rollout_images:\n-        video_writer.append_data(img)\n-    video_writer.close()\n-    print(f\"Saved rollout MP4 at path {mp4_path}\")\n-    if log_file is not None:\n-        log_file.write(f\"Saved rollout MP4 at path {mp4_path}\\n\")\n-    return mp4_path\n-\n-\n-def quat2axisangle(quat):\n-    \"\"\"\n-    Copied from robosuite: https://github.com/ARISE-Initiative/robosuite/blob/\u001b[93meafb81f54ffc104f905ee48a16bb15f059176ad3\u001b[0m/robosuite/utils/transform_utils.py#L490C1-L512C55\n-\n-    Converts quaternion to axis-angle format.\n-    Returns a unit vector direction scaled by its angle in radians.\n-\n-    Args:\n-        quat (np.array): (x,y,z,w) vec4 float angles\n-\n-    Returns:\n-        np.array: (ax,ay,az) axis-angle exponential coordinates\n-    \"\"\"\n-    # clip quaternion\n-    if quat[3] > 1.0:\n-        quat[3] = 1.0\n-    elif quat[3] < -1.0:\n-        quat[3] = -1.0\n-\n-    den = np.sqrt(1.0 - quat[3] * quat[3])\n-    if math.isclose(den, 0.0):\n-        # This is (close to) a zero degree rotation, immediately return\n-        return np.zeros(3)\n-\n-    return (quat[:3] * 2.0 * math.acos(quat[3])) / den\n",
  "reason": "High Entropy",
  "stringsFound": [
    "eafb81f54ffc104f905ee48a16bb15f059176ad3"
  ]
}
----------------------------------------
Secret #2:
{
  "branch": "origin/main",
  "commit": "Update dlimp package source\n\nUsing Moo Jin's fork: https://github.com/moojink/dlimp_openvla\nSets options.deterministic = True in RLDS dataloader.\n",
  "commitHash": "317dbd9e41edaa83ddf2b33f5412b5e7ddadcd4e",
  "date": "2024-07-15 22:30:37",
  "diff": "@@ -415,7 +415,7 @@ FileNotFoundError: Failed to construct dataset \"fractal20220817_data\", builder_k\n AttributeError: 'DLataset' object has no attribute 'traj_map'. Did you mean: 'flat_map'?\n ```\n - **Fix**: Upgrade `dlimp` to the newest version. You may have to `--force-reinstall` like so:\n-`pip install --no-deps --force-reinstall git+https://github.com/moojink/dlimp_openvla`\n+`pip install --no-deps --force-reinstall git+https://github.com/kvablack/dlimp@5edaa4691567873d495633f2708982b42edf1972`\n \n ---\n \n",
  "path": "README.md",
  "printDiff": "@@ -415,7 +415,7 @@ FileNotFoundError: Failed to construct dataset \"fractal20220817_data\", builder_k\n AttributeError: 'DLataset' object has no attribute 'traj_map'. Did you mean: 'flat_map'?\n ```\n - **Fix**: Upgrade `dlimp` to the newest version. You may have to `--force-reinstall` like so:\n-`pip install --no-deps --force-reinstall git+https://github.com/moojink/dlimp_openvla`\n+`pip install --no-deps --force-reinstall git+https://github.com/kvablack/dlimp@\u001b[93m5edaa4691567873d495633f2708982b42edf1972\u001b[0m`\n \n ---\n \n",
  "reason": "High Entropy",
  "stringsFound": [
    "5edaa4691567873d495633f2708982b42edf1972"
  ]
}
----------------------------------------
Secret #3:
{
  "branch": "origin/main",
  "commit": "Update dlimp package source\n\nUsing Moo Jin's fork: https://github.com/moojink/dlimp_openvla\nSets options.deterministic = True in RLDS dataloader.\n",
  "commitHash": "317dbd9e41edaa83ddf2b33f5412b5e7ddadcd4e",
  "date": "2024-07-15 22:30:37",
  "diff": "@@ -52,7 +52,7 @@ dependencies = [\n     \"tensorflow==2.15.0\",\n     \"tensorflow_datasets==4.9.3\",\n     \"tensorflow_graphics==2021.12.3\",\n-    \"dlimp @ git+https://github.com/moojink/dlimp_openvla\"\n+    \"dlimp @ git+https://github.com/kvablack/dlimp@ad72ce3a9b414db2185bc0b38461d4101a65477a\"\n ]\n \n [project.optional-dependencies]\n",
  "path": "pyproject.toml",
  "printDiff": "@@ -52,7 +52,7 @@ dependencies = [\n     \"tensorflow==2.15.0\",\n     \"tensorflow_datasets==4.9.3\",\n     \"tensorflow_graphics==2021.12.3\",\n-    \"dlimp @ git+https://github.com/moojink/dlimp_openvla\"\n+    \"dlimp @ git+https://github.com/kvablack/dlimp@\u001b[93mad72ce3a9b414db2185bc0b38461d4101a65477a\u001b[0m\"\n ]\n \n [project.optional-dependencies]\n",
  "reason": "High Entropy",
  "stringsFound": [
    "ad72ce3a9b414db2185bc0b38461d4101a65477a"
  ]
}
----------------------------------------
Secret #4:
{
  "branch": "origin/main",
  "commit": "Update README: newer dlimp version\n",
  "commitHash": "4a2d356473991b4fcd8dd839d2a4823c76abf7fb",
  "date": "2024-06-14 07:30:31",
  "diff": "@@ -170,7 +170,7 @@ FileNotFoundError: Failed to construct dataset \"fractal20220817_data\", builder_k\n AttributeError: 'DLataset' object has no attribute 'traj_map'. Did you mean: 'flat_map'?\n ```\n - **Fix**: Upgrade `dlimp` to the newest version. You may have to `--force-reinstall` like so:\n-`pip install --no-deps --force-reinstall git+https://github.com/kvablack/dlimp@5edaa4691567873d495633f2708982b42edf1972`\n+`pip install --no-deps --force-reinstall git+https://github.com/kvablack/dlimp@ad72ce3a9b414db2185bc0b38461d4101a65477a`\n \n ---\n \n",
  "path": "README.md",
  "printDiff": "@@ -170,7 +170,7 @@ FileNotFoundError: Failed to construct dataset \"fractal20220817_data\", builder_k\n AttributeError: 'DLataset' object has no attribute 'traj_map'. Did you mean: 'flat_map'?\n ```\n - **Fix**: Upgrade `dlimp` to the newest version. You may have to `--force-reinstall` like so:\n-`pip install --no-deps --force-reinstall git+https://github.com/kvablack/dlimp@\u001b[93m5edaa4691567873d495633f2708982b42edf1972\u001b[0m`\n+`pip install --no-deps --force-reinstall git+https://github.com/kvablack/dlimp@\u001b[93mad72ce3a9b414db2185bc0b38461d4101a65477a\u001b[0m`\n \n ---\n \n",
  "reason": "High Entropy",
  "stringsFound": [
    "5edaa4691567873d495633f2708982b42edf1972",
    "ad72ce3a9b414db2185bc0b38461d4101a65477a"
  ]
}
----------------------------------------
Secret #5:
{
  "branch": "origin/main",
  "commit": "OpenVLA Release\n",
  "commitHash": "959c1d5848c3b396fb7e57cd32b80aa76544486e",
  "date": "2024-06-13 21:48:51",
  "diff": "@@ -1,111 +1,42 @@\n-# OpenVLA - Vision-Language-Action Models for Robotics\n+# Prismatic VLMs\n \n-[![arXiv](https://img.shields.io/badge/arXiv-2406.07210-df2a2a.svg?style=for-the-badge)](https://openvla.github.io/openvla.pdf)\n-[![HF Models](https://img.shields.io/badge/%F0%9F%A4%97-Models-yellow?style=for-the-badge)](https://huggingface.co/openvla/openvla-7b)\n-[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.0-EE4C2C.svg?style=for-the-badge&logo=pytorch)](https://pytorch.org/get-started/locally/)\n+[![arXiv](https://img.shields.io/badge/arXiv-2402.07865-df2a2a.svg?style=for-the-badge)](https://arxiv.org/abs/2402.07865)\n+[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.1-EE4C2C.svg?style=for-the-badge&logo=pytorch)](https://pytorch.org/get-started/locally/)\n [![Python](https://img.shields.io/badge/python-3.10-blue?style=for-the-badge)](https://www.python.org)\n [![License](https://img.shields.io/github/license/TRI-ML/prismatic-vlms?style=for-the-badge)](LICENSE)\n- \n-[**Getting Started**](#getting-started) | [**Pretrained VLAs**](#pretrained-vlas) | [**Installation**](#installation) | \n-[**Training VLAs from Scratch**](#training-vlas-from-scratch) | [**Project Page**](https://openvla.github.io/)\n \n-A simple and scalable codebase for training and fine-tuning vision-language-action models (VLAs) for generalist robotic \n-manipulation:\n+[**Installation**](#installation) | [**Usage**](#usage) | [**Pretrained Models**](#pretrained-models) | [**Training VLMs**](#training-vlms)\n \n-- **Different Dataset Mixtures**: We natively support arbitrary datasets in RLDS format, including arbitrary mixtures of\n-  data from Open X-Embodiment.\n-- **Easy Scaling**: Powered by PyTorch FSDP and Flash-Attention, we can quickly and efficiently train models from 1B - \n-  34B parameters, with easily adaptable model architectures.\n-- **Native Fine-Tuning Support**: Built-in support (with examples) for various forms of fine-tuning (full, \n-  partial, LoRA).\n+A flexible and efficient codebase for training visually-conditioned language-models (VLMs):\n \n-Built on top of [Prismatic VLMs](https://github.com/TRI-ML/prismatic-vlms).\n+- **Different Visual Representations**. We natively support backbones such as [CLIP](https://arxiv.org/abs/2103.00020), \n+  [SigLIP](https://arxiv.org/abs/2303.15343), [DINOv2](https://arxiv.org/abs/2304.07193) \u2013 and even fusions of different backbones. \n+  Adding new backbones is easy via [TIMM](https://huggingface.co/timm).\n+- **Base and Instruct-Tuned Language Models**. We support arbitrary instances of `AutoModelForCausalLM` including both \n+  base and instruct-tuned models (with built-in prompt handling) via [Transformers](https://github.com/huggingface/transformers). \n+  If your favorite LM isn't already supported, feel free to submit a PR!\n+- **Easy Scaling**. Powered by PyTorch FSDP and Flash-Attention, we can quickly and efficiently train models from 1B - \n+  34B parameters, on different, easily configurable dataset mixtures.\n \n-## Getting Started\n-\n-To get started with loading and running OpenVLA models for inference, we provide a lightweight interface that leverages\n-HuggingFace `transformers` AutoClasses, with minimal dependencies.\n-\n-For example, to load `openvla-7b` for zero-shot instruction following in the\n-[Bridge V2 environments](https://rail-berkeley.github.io/bridgedata/) with a Widow-X robot:\n-\n-```python\n-# Install minimal dependencies (`torch`, `transformers`, `timm`, `tokenizers`, ...)\n-# > pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt\n-from transformers import AutoModelForVision2Seq, AutoProcessor\n-from PIL import Image\n-\n-import torch\n-\n-# Load Processor & VLA\n-processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n-vla = AutoModelForVision2Seq.from_pretrained(\n-    \"openvla/openvla-7b\", \n-    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n-    torch_dtype=torch.bfloat16, \n-    low_cpu_mem_usage=True, \n-    trust_remote_code=True\n-).to(\"cuda:0\")\n-\n-# Grab image input & format prompt\n-image: Image.Image = get_from_camera(...)\n-prompt = \"In: What action should the robot take to {<INSTRUCTION>}?\\nOut:\"\n-\n-# Predict Action (7-DoF; un-normalize for BridgeV2)\n-inputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\n-action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n-\n-# Execute...\n-robot.act(action, ...)\n-```\n-\n-We also provide an [example script for fine-tuning OpenVLA models for new tasks and \n-embodiments](./vla-scripts/finetune.py); this script supports different fine-tuning modes -- including (quantized) \n-low-rank adaptation (LoRA) supported by [HuggingFace's PEFT library](https://huggingface.co/docs/peft/en/index). \n-\n-For deployment, we provide a lightweight script for [serving OpenVLA models over a REST API](./vla-scripts/deploy.py), \n-providing an easy way to integrate OpenVLA models into existing robot control stacks, \n-removing any requirement for powerful on-device compute.\n-\n-## Pretrained VLAs\n-\n-We release two OpenVLA models trained as part of our work, with checkpoints, configs, and model cards available [on our\n-HuggingFace page](https://huggingface.co/openvla):\n-- [`openvla-7b`](https://huggingface.co/openvla/openvla-7b): The flagship model from our paper, trained from \n-  the Prismatic `prism-dinosiglip-224px` VLM (based on a fused DINOv2 and SigLIP vision backbone, and Llama-2 LLM). \n-  Trained on a large mixture of datasets from Open X-Embodiment spanning 970K trajectories \n-  ([mixture details](./prismatic/vla/datasets/rlds/oxe/mixtures.py)).\n-- [`openvla-v01-7b`](https://huggingface.co/openvla/openvla-7b-v01): An early model used during development, trained from\n-  the Prismatic `siglip-224px` VLM (singular SigLIP vision backbone, and a Vicu\u00f1a v1.5 LLM). Trained on the same mixture\n-  of datasets as [Octo](https://github.com/octo-models/octo), but for significantly fewer GPU hours than our final model \n-  ([mixture details](./prismatic/vla/datasets/rlds/oxe/mixtures.py)).\n-\n-**Explicit Notes on Model Licensing & Commercial Use**: While all code in this repository is released under an MIT \n-License, our pretrained models may inherit restrictions from the underlying base models we use. Specifically, both the\n-above models are derived from Llama-2, and as such are subject to the \n-[Llama Community License](https://ai.meta.com/llama/license/).\n+If you're interested in rigorously evaluating existing VLMs, check our [evaluation codebase](https://github.com/TRI-ML/vlm-evaluation)\n+that bundles together 12 different battle-tested vision-and-language benchmarks through a clean, automated test harness. \n \n ---\n \n ## Installation\n \n-> **Note**: These installation instructions are for full-scale pretraining (and distributed fine-tuning); if looking to\n-  just run inference with OpenVLA models (or perform lightweight fine-tuning), see instructions above!\n-\n This repository was built using Python 3.10, but should be backwards compatible with any Python >= 3.8. We require\n-PyTorch 2.2.* -- installation instructions [can be found here](https://pytorch.org/get-started/locally/). The latest \n-version of this repository was developed and thoroughly tested with:\n-  - PyTorch 2.2.0, torchvision 0.17.0, transformers 4.40.1, tokenizers 0.19.1, timm 0.9.10, and flash-attn 2.5.5\n-\n-**[5/21/24] Note**: Following reported regressions and breaking changes in later versions of `transformers`, `timm`, and\n-`tokenizers` we explicitly pin the above versions of the dependencies. We are working on implementing thorough tests, \n-and plan on relaxing these constraints as soon as we can.\n+PyTorch 2.1 or greater -- installation instructions [can be found here](https://pytorch.org/get-started/locally/). This \n+repository was developed and has been thoroughly tested with:\n+  - [2/16/24] PyTorch 2.1.0, Torchvision 0.16.0, Transformers 4.34.1, and Flash-Attention 2.3.3.\n+  - [3/24/24] PyTorch 2.2.1, Torchvision 0.17.0, Transformers 4.38.1, and Flash-Attention 2.5.5.\n \n-Once PyTorch has been properly installed, you can install this package locally via an editable installation (or via \n-`pip install git+https://github.com/openvla/openvla`):\n+Once PyTorch has been properly installed, you can install this package locally via an editable installation (or via\n+`pip install git+https://github.com/TRI-ML/prismatic-vlms`):\n \n ```bash\n-cd openvla\n+git clone https://github.com/TRI-ML/prismatic-vlms\n+cd prismatic-vlms\n pip install -e .\n \n # Training additionally requires Flash-Attention 2 (https://github.com/Dao-AILab/flash-attention)\n@@ -114,63 +45,149 @@ pip install packaging ninja\n # Verify Ninja --> should return exit code \"0\"\n ninja --version; echo $?\n \n-# Install Flash Attention 2\n+# Install Flash Attention 2 \n #   =>> If you run into difficulty, try `pip cache remove flash_attn` first\n-pip install \"flash-attn==2.5.5\" --no-build-isolation\n+pip install flash-attn --no-build-isolation\n ```\n \n If you run into any problems during the installation process, please file a GitHub Issue.\n \n-## Training VLAs from Scratch\n+## Usage\n \n-We provide full instructions and configurations for training OpenVLA models on (arbitrary subsets of) the\n-[Open X-Embodiment (OXE) Dataset](https://robotics-transformer-x.github.io/). If you run in to any issues with \n-the following, see [VLA Troubleshooting](#vla-troubleshooting) below (or file a GitHub Issue).\n+Once installed, loading and running inference with pretrained `prismatic` models is easy:\n \n-### VLA Pretraining Datasets\n+```python\n+import requests\n+import torch\n \n-We download and preprocess individual datasets from Open X-Embodiment in [RLDS format](https://github.com/google-research/rlds) following \n-[this custom script](https://github.com/kpertsch/rlds_dataset_mod/blob/main/prepare_open_x.sh). See \n-[mixtures.py](./prismatic/vla/datasets/rlds/oxe/mixtures.py) for the full list of component datasets (and mixture \n-weights) we use to train `openvla-7b`. \n-- **Important**: For the Bridge V2 component dataset, the version in OXE is out of date (as of 12/20/2023). Instead,\n-  you should download the dataset from the [official website](https://rail.eecs.berkeley.edu/datasets/bridge_release/data/tfds/bridge_dataset/) and place it under the subdirectory `bridge_orig/`. \n-  Replace any reference to `bridge` in the OXE code with `bridge_orig`.\n+from PIL import Image\n+from pathlib import Path\n+\n+from prismatic import load\n+\n+# For gated LMs like Llama-2, make sure to request official access, and generate an access token\n+hf_token = Path(\".hf_token\").read_text().strip()\n+device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n+\n+# Load a pretrained VLM (either local path, or ID to auto-download from the HF Hub) \n+model_id = \"prism-dinosiglip+7b\"\n+vlm = load(model_id, hf_token=hf_token)\n+vlm.to(device, dtype=torch.bfloat16)\n+\n+# Download an image and specify a prompt\n+image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\"\n+image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n+user_prompt = \"What is going on in this image?\"\n+\n+# Build prompt\n+prompt_builder = vlm.get_prompt_builder()\n+prompt_builder.add_turn(role=\"human\", message=user_prompt)\n+prompt_text = prompt_builder.get_prompt()\n+\n+# Generate!\n+generated_text = vlm.generate(\n+    image,\n+    prompt_text,\n+    do_sample=True,\n+    temperature=0.4,\n+    max_new_tokens=512,\n+    min_length=1,\n+)\n+```\n \n-### VLA Configuration & Training Script\n+For a complete terminal-based CLI for interacting with our VLMs, check out [scripts/generate.py](scripts/generate.py). \n \n-The entry point for VLA training is [`vla-scripts/train.py`](vla-scripts/train.py). We use \n-[`draccus`](https://pypi.org/project/draccus) to provide a modular, dataclass-based interface for specifying VLA \n-training configurations; existing VLA configurations are in [`prismatic/conf/vla.py`](prismatic/conf/vla.py). You can \n-add your own training configuration and refer to it using the `--vla.type` command line argument.\n+## Pretrained Models\n \n-We use PyTorch Fully Sharded Data Parallel (FSDP) to distribute training across GPUs. Launch training via `torchrun`:\n+We release **all 49** VLMs trained as part of our work, with a range of different visual representations, language\n+models, data, and scale. The exhaustive set of models (with structured descriptions) can be found in \n+[`prismatic/models/registry.py](prismatic/models/registry.py) - we will continue to update this registry as we train\n+additional models.\n \n-```bash\n-# Train VLA on Bridge V2 with the Prismatic SigLIP 224px Backbone on a Single Node (w/ 8 GPUs)\n-torchrun --standalone --nnodes 1 --nproc-per-node 8 vla-scripts/train.py \\\n-  --vla.type \"siglip-224px+mx-bridge\" \\\n-  --data_root_dir <PATH TO OXE DATA ROOT> \\\n-  --run_root_dir <PATH TO LOG/CHECKPOINT ROOT> \\\n-  --wandb_project \"<PROJECT>\" \\\n-  --wandb_entity \"<ENTITY>\"\n+We also provide a top-level API for instantiating models from the names mentioned in the various Figures of our paper,\n+as well as for generally browsing our pretrained models by description:\n+\n+```python\n+from prismatic import available_model_ids_and_names, available_model_ids, get_model_description\n+from pprint import pprint\n+\n+# List all Pretrained VLMs (by HF Hub IDs)\n+pprint(available_model_ids())\n+\n+# List all Pretrained VLMs with both HF Hub IDs AND semantically meaningful names from paper\n+pprint(available_model_ids_and_names())\n+\n+# Print and return a targeted description of a model (by name or ID) \n+#   =>> See `prismatic/models/registry.py` for explicit schema\n+description = get_model_description(\"Prism-DINOSigLIP 13B (Controlled)\")\n ```\n \n-### VLA Troubleshooting\n+Currently, our best performing models are the `Prism-DINOSigLIP` series, with especially strong performance on spatial\n+understanding and localization tasks.\n+\n+---\n+**Explicit Notes on Model Licensing & Commercial Use**: While all code in this repository is released under an MIT\n+License, our pretrained models may inherit restrictions from the _datasets_ and _underlying LMs_ we use for training. \n+\n+**[02/09/24]** Our current VLMs are all derived from Llama-2, and as such are subject to the \n+[Llama Community License](https://ai.meta.com/llama/license/), which does permit commercial use. We additionally train \n+on the LLaVa Instruct Tuning data.\n+\n+**[05/05/24]** Our new VLMs derived from Mistral and Phi-2 are subject to the original Apache and MIT Licenses attached\n+to each model.\n+\n+As we train new models, we will update this section of the README (and the LICENSE files associated with each model)\n+appropriately. If there are any questions, please file an Issue!\n \n-The following are a list of known problems and corresponding fixes:\n+## Training VLMs\n+\n+In addition to providing all pretrained VLMs trained in this work, we also provide full instructions and configurations\n+for _reproducing all results_ (down to controlling for the batch order of examples seen during training). \n+\n+#### Pretraining Datasets\n+For the [LLaVa v1.5 Instruct Dataset](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md) we use for all \n+of our models, we provide an automated download script in [`scripts/preprocess.py`](scripts/preprocess.py):\n \n ```bash\n-FileNotFoundError: Failed to construct dataset \"fractal20220817_data\", builder_kwargs \"{'data_dir': '/path/to/processed/datasets/'}\": Could not load dataset info from fractal20220817_data/0.1.0/dataset_info.json\n+# Download the `llava-v1.5-instruct` (Instruct Tuning) Image and Language Data (includes extra post-processing)\n+python scripts/preprocess.py --dataset_id \"llava-v1.5-instruct\" --root_dir <PATH-TO-DATA-ROOT>\n+\n+# (In case you also wish to download the explicit vision-language alignment data)\n+python scripts/preprocess.py --dataset_id \"llava-laion-cc-sbu-558k\" --root_dir <PATH-TO-DATA-ROOT>\n ```\n-- **Fix**: Downgrade `tensorflow-datasets` via `pip install tensorflow-datasets==4.9.3`.\n \n+As part of our work, we also train on mixtures of datasets including \n+[LVIS-Instruct-4V](https://arxiv.org/abs/2311.07574) and [LRV-Instruct](https://arxiv.org/abs/2306.14565). We provide\n+instructions and scripts for downloading these datasets in [`scripts/additional-datasets`](scripts/additional-datasets).\n+\n+We welcome any and all contributions and pull requests to add new datasets!\n+\n+#### Model Configuration & Training Script\n+\n+The entry point for training models is [`scripts/pretrain.py`](scripts/pretrain.py). We employ \n+[`draccus`](https://pypi.org/project/draccus/0.6/) to provide a modular, dataclass-based interface for specifying \n+model configurations; all 42 VLM configurations are in [`prismatic/conf/models.py`](prismatic/conf/models.py). \n+\n+We use PyTorch Fully Sharded Data Parallel (FSDP) to distribute training across GPUs, though we also provide a simpler\n+Distributed Data Parallel training implementation (for smaller LM backbones, debugging). You can run a pretraining job\n+via `torchrun`.\n+\n+As a compact example, here's how you would train a VLM derived from Vicu\u00f1a-v1.5 7B, using fused DINOv2 + SigLIP \n+representations, processing non-square images with a \"letterbox padding\" transform across 8 GPUs on a single-node: \n \n ```bash\n-AttributeError: 'DLataset' object has no attribute 'traj_map'. Did you mean: 'flat_map'?\n+# Run from the root of the repository\n+torchrun --standalone --nnodes 1 --nproc-per-node 8 scripts/pretrain.py \\\n+  --model.type \"one-stage+7b\" \\\n+  --model.model_id \"<NAME OF NEW MODEL>\" \\\n+  --model.vision_backbone_id \"dinosiglip-vit-so-384px\" \\\n+  --model.image_resize_strategy \"letterbox\" \\\n+  --model.llm_backbone_id \"vicuna-v15-7b\" \n ```\n-- **Fix**: Upgrade `dlimp` to the newest version. You may have to `--force-reinstall` like so:\n-`pip install --no-deps --force-reinstall git+https://github.com/kvablack/dlimp@ad72ce3a9b414db2185bc0b38461d4101a65477a`\n+\n+Note that specifying `model.type` is important for identifying the _base configuration_ that you want to build on top of;\n+the full list of model types are available in our [config file](prismatic/conf/models.py), under the `model_id` key for \n+each dataclass.\n \n ---\n \n@@ -179,7 +196,7 @@ AttributeError: 'DLataset' object has no attribute 'traj_map'. Did you mean: 'fl\n High-level overview of repository/project file-tree:\n \n + `prismatic` - Package source; provides core utilities for model loading, training, data preprocessing, etc.\n-+ `vla-scripts/` - Core scripts for training, fine-tuning, and deploying VLAs.\n++ `scripts/` - Standalone scripts for preprocessing, training VLMs, and generating from pretrained models.\n + `LICENSE` - All code is made available under the MIT License; happy hacking!\n + `Makefile` - Top-level Makefile (by default, supports linting - checking & auto-fix); extend as needed.\n + `pyproject.toml` - Full project configuration details (including dependencies), as well as tool configurations.\n@@ -187,15 +204,15 @@ High-level overview of repository/project file-tree:\n \n ---\n \n-#### Citation\n+#### Citation \n \n-If you find our code or models useful in your work, please cite [our paper](https://openvla.github.io/openvla.pdf):\n+If you find our code or models useful in your work, please cite [our paper](https://arxiv.org/abs/2402.07865):\n \n ```bibtex\n-@article{kim24openvla,\n-    title={OpenVLA: An Open-Source Vision-Language-Action Model},\n-    author={{Moo Jin} Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn},\n-    journal = {arXiv preprint},\n-    year={2024}\n-} \n-```\n\\ No newline at end of file\n+@inproceedings{karamcheti2024prismatic,\n+  title = {Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models},\n+  author = {Siddharth Karamcheti and Suraj Nair and Ashwin Balakrishna and Percy Liang and Thomas Kollar and Dorsa Sadigh},\n+  booktitle = {International Conference on Machine Learning (ICML)},\n+  year = {2024},\n+}\n+```\n",
  "path": "README.md",
  "printDiff": "@@ -1,111 +1,42 @@\n-# OpenVLA - Vision-Language-Action Models for Robotics\n+# Prismatic VLMs\n \n-[![arXiv](https://img.shields.io/badge/arXiv-2406.07210-df2a2a.svg?style=for-the-badge)](https://openvla.github.io/openvla.pdf)\n-[![HF Models](https://img.shields.io/badge/%F0%9F%A4%97-Models-yellow?style=for-the-badge)](https://huggingface.co/openvla/openvla-7b)\n-[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.0-EE4C2C.svg?style=for-the-badge&logo=pytorch)](https://pytorch.org/get-started/locally/)\n+[![arXiv](https://img.shields.io/badge/arXiv-2402.07865-df2a2a.svg?style=for-the-badge)](https://arxiv.org/abs/2402.07865)\n+[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.1-EE4C2C.svg?style=for-the-badge&logo=pytorch)](https://pytorch.org/get-started/locally/)\n [![Python](https://img.shields.io/badge/python-3.10-blue?style=for-the-badge)](https://www.python.org)\n [![License](https://img.shields.io/github/license/TRI-ML/prismatic-vlms?style=for-the-badge)](LICENSE)\n- \n-[**Getting Started**](#getting-started) | [**Pretrained VLAs**](#pretrained-vlas) | [**Installation**](#installation) | \n-[**Training VLAs from Scratch**](#training-vlas-from-scratch) | [**Project Page**](https://openvla.github.io/)\n \n-A simple and scalable codebase for training and fine-tuning vision-language-action models (VLAs) for generalist robotic \n-manipulation:\n+[**Installation**](#installation) | [**Usage**](#usage) | [**Pretrained Models**](#pretrained-models) | [**Training VLMs**](#training-vlms)\n \n-- **Different Dataset Mixtures**: We natively support arbitrary datasets in RLDS format, including arbitrary mixtures of\n-  data from Open X-Embodiment.\n-- **Easy Scaling**: Powered by PyTorch FSDP and Flash-Attention, we can quickly and efficiently train models from 1B - \n-  34B parameters, with easily adaptable model architectures.\n-- **Native Fine-Tuning Support**: Built-in support (with examples) for various forms of fine-tuning (full, \n-  partial, LoRA).\n+A flexible and efficient codebase for training visually-conditioned language-models (VLMs):\n \n-Built on top of [Prismatic VLMs](https://github.com/TRI-ML/prismatic-vlms).\n+- **Different Visual Representations**. We natively support backbones such as [CLIP](https://arxiv.org/abs/2103.00020), \n+  [SigLIP](https://arxiv.org/abs/2303.15343), [DINOv2](https://arxiv.org/abs/2304.07193) \u2013 and even fusions of different backbones. \n+  Adding new backbones is easy via [TIMM](https://huggingface.co/timm).\n+- **Base and Instruct-Tuned Language Models**. We support arbitrary instances of `AutoModelForCausalLM` including both \n+  base and instruct-tuned models (with built-in prompt handling) via [Transformers](https://github.com/huggingface/transformers). \n+  If your favorite LM isn't already supported, feel free to submit a PR!\n+- **Easy Scaling**. Powered by PyTorch FSDP and Flash-Attention, we can quickly and efficiently train models from 1B - \n+  34B parameters, on different, easily configurable dataset mixtures.\n \n-## Getting Started\n-\n-To get started with loading and running OpenVLA models for inference, we provide a lightweight interface that leverages\n-HuggingFace `transformers` AutoClasses, with minimal dependencies.\n-\n-For example, to load `openvla-7b` for zero-shot instruction following in the\n-[Bridge V2 environments](https://rail-berkeley.github.io/bridgedata/) with a Widow-X robot:\n-\n-```python\n-# Install minimal dependencies (`torch`, `transformers`, `timm`, `tokenizers`, ...)\n-# > pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt\n-from transformers import AutoModelForVision2Seq, AutoProcessor\n-from PIL import Image\n-\n-import torch\n-\n-# Load Processor & VLA\n-processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n-vla = AutoModelForVision2Seq.from_pretrained(\n-    \"openvla/openvla-7b\", \n-    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n-    torch_dtype=torch.bfloat16, \n-    low_cpu_mem_usage=True, \n-    trust_remote_code=True\n-).to(\"cuda:0\")\n-\n-# Grab image input & format prompt\n-image: Image.Image = get_from_camera(...)\n-prompt = \"In: What action should the robot take to {<INSTRUCTION>}?\\nOut:\"\n-\n-# Predict Action (7-DoF; un-normalize for BridgeV2)\n-inputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\n-action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n-\n-# Execute...\n-robot.act(action, ...)\n-```\n-\n-We also provide an [example script for fine-tuning OpenVLA models for new tasks and \n-embodiments](./vla-scripts/finetune.py); this script supports different fine-tuning modes -- including (quantized) \n-low-rank adaptation (LoRA) supported by [HuggingFace's PEFT library](https://huggingface.co/docs/peft/en/index). \n-\n-For deployment, we provide a lightweight script for [serving OpenVLA models over a REST API](./vla-scripts/deploy.py), \n-providing an easy way to integrate OpenVLA models into existing robot control stacks, \n-removing any requirement for powerful on-device compute.\n-\n-## Pretrained VLAs\n-\n-We release two OpenVLA models trained as part of our work, with checkpoints, configs, and model cards available [on our\n-HuggingFace page](https://huggingface.co/openvla):\n-- [`openvla-7b`](https://huggingface.co/openvla/openvla-7b): The flagship model from our paper, trained from \n-  the Prismatic `prism-dinosiglip-224px` VLM (based on a fused DINOv2 and SigLIP vision backbone, and Llama-2 LLM). \n-  Trained on a large mixture of datasets from Open X-Embodiment spanning 970K trajectories \n-  ([mixture details](./prismatic/vla/datasets/rlds/oxe/mixtures.py)).\n-- [`openvla-v01-7b`](https://huggingface.co/openvla/openvla-7b-v01): An early model used during development, trained from\n-  the Prismatic `siglip-224px` VLM (singular SigLIP vision backbone, and a Vicu\u00f1a v1.5 LLM). Trained on the same mixture\n-  of datasets as [Octo](https://github.com/octo-models/octo), but for significantly fewer GPU hours than our final model \n-  ([mixture details](./prismatic/vla/datasets/rlds/oxe/mixtures.py)).\n-\n-**Explicit Notes on Model Licensing & Commercial Use**: While all code in this repository is released under an MIT \n-License, our pretrained models may inherit restrictions from the underlying base models we use. Specifically, both the\n-above models are derived from Llama-2, and as such are subject to the \n-[Llama Community License](https://ai.meta.com/llama/license/).\n+If you're interested in rigorously evaluating existing VLMs, check our [evaluation codebase](https://github.com/TRI-ML/vlm-evaluation)\n+that bundles together 12 different battle-tested vision-and-language benchmarks through a clean, automated test harness. \n \n ---\n \n ## Installation\n \n-> **Note**: These installation instructions are for full-scale pretraining (and distributed fine-tuning); if looking to\n-  just run inference with OpenVLA models (or perform lightweight fine-tuning), see instructions above!\n-\n This repository was built using Python 3.10, but should be backwards compatible with any Python >= 3.8. We require\n-PyTorch 2.2.* -- installation instructions [can be found here](https://pytorch.org/get-started/locally/). The latest \n-version of this repository was developed and thoroughly tested with:\n-  - PyTorch 2.2.0, torchvision 0.17.0, transformers 4.40.1, tokenizers 0.19.1, timm 0.9.10, and flash-attn 2.5.5\n-\n-**[5/21/24] Note**: Following reported regressions and breaking changes in later versions of `transformers`, `timm`, and\n-`tokenizers` we explicitly pin the above versions of the dependencies. We are working on implementing thorough tests, \n-and plan on relaxing these constraints as soon as we can.\n+PyTorch 2.1 or greater -- installation instructions [can be found here](https://pytorch.org/get-started/locally/). This \n+repository was developed and has been thoroughly tested with:\n+  - [2/16/24] PyTorch 2.1.0, Torchvision 0.16.0, Transformers 4.34.1, and Flash-Attention 2.3.3.\n+  - [3/24/24] PyTorch 2.2.1, Torchvision 0.17.0, Transformers 4.38.1, and Flash-Attention 2.5.5.\n \n-Once PyTorch has been properly installed, you can install this package locally via an editable installation (or via \n-`pip install git+https://github.com/openvla/openvla`):\n+Once PyTorch has been properly installed, you can install this package locally via an editable installation (or via\n+`pip install git+https://github.com/TRI-ML/prismatic-vlms`):\n \n ```bash\n-cd openvla\n+git clone https://github.com/TRI-ML/prismatic-vlms\n+cd prismatic-vlms\n pip install -e .\n \n # Training additionally requires Flash-Attention 2 (https://github.com/Dao-AILab/flash-attention)\n@@ -114,63 +45,149 @@ pip install packaging ninja\n # Verify Ninja --> should return exit code \"0\"\n ninja --version; echo $?\n \n-# Install Flash Attention 2\n+# Install Flash Attention 2 \n #   =>> If you run into difficulty, try `pip cache remove flash_attn` first\n-pip install \"flash-attn==2.5.5\" --no-build-isolation\n+pip install flash-attn --no-build-isolation\n ```\n \n If you run into any problems during the installation process, please file a GitHub Issue.\n \n-## Training VLAs from Scratch\n+## Usage\n \n-We provide full instructions and configurations for training OpenVLA models on (arbitrary subsets of) the\n-[Open X-Embodiment (OXE) Dataset](https://robotics-transformer-x.github.io/). If you run in to any issues with \n-the following, see [VLA Troubleshooting](#vla-troubleshooting) below (or file a GitHub Issue).\n+Once installed, loading and running inference with pretrained `prismatic` models is easy:\n \n-### VLA Pretraining Datasets\n+```python\n+import requests\n+import torch\n \n-We download and preprocess individual datasets from Open X-Embodiment in [RLDS format](https://github.com/google-research/rlds) following \n-[this custom script](https://github.com/kpertsch/rlds_dataset_mod/blob/main/prepare_open_x.sh). See \n-[mixtures.py](./prismatic/vla/datasets/rlds/oxe/mixtures.py) for the full list of component datasets (and mixture \n-weights) we use to train `openvla-7b`. \n-- **Important**: For the Bridge V2 component dataset, the version in OXE is out of date (as of 12/20/2023). Instead,\n-  you should download the dataset from the [official website](https://rail.eecs.berkeley.edu/datasets/bridge_release/data/tfds/bridge_dataset/) and place it under the subdirectory `bridge_orig/`. \n-  Replace any reference to `bridge` in the OXE code with `bridge_orig`.\n+from PIL import Image\n+from pathlib import Path\n+\n+from prismatic import load\n+\n+# For gated LMs like Llama-2, make sure to request official access, and generate an access token\n+hf_token = Path(\".hf_token\").read_text().strip()\n+device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n+\n+# Load a pretrained VLM (either local path, or ID to auto-download from the HF Hub) \n+model_id = \"prism-dinosiglip+7b\"\n+vlm = load(model_id, hf_token=hf_token)\n+vlm.to(device, dtype=torch.bfloat16)\n+\n+# Download an image and specify a prompt\n+image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\"\n+image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n+user_prompt = \"What is going on in this image?\"\n+\n+# Build prompt\n+prompt_builder = vlm.get_prompt_builder()\n+prompt_builder.add_turn(role=\"human\", message=user_prompt)\n+prompt_text = prompt_builder.get_prompt()\n+\n+# Generate!\n+generated_text = vlm.generate(\n+    image,\n+    prompt_text,\n+    do_sample=True,\n+    temperature=0.4,\n+    max_new_tokens=512,\n+    min_length=1,\n+)\n+```\n \n-### VLA Configuration & Training Script\n+For a complete terminal-based CLI for interacting with our VLMs, check out [scripts/generate.py](scripts/generate.py). \n \n-The entry point for VLA training is [`vla-scripts/train.py`](vla-scripts/train.py). We use \n-[`draccus`](https://pypi.org/project/draccus) to provide a modular, dataclass-based interface for specifying VLA \n-training configurations; existing VLA configurations are in [`prismatic/conf/vla.py`](prismatic/conf/vla.py). You can \n-add your own training configuration and refer to it using the `--vla.type` command line argument.\n+## Pretrained Models\n \n-We use PyTorch Fully Sharded Data Parallel (FSDP) to distribute training across GPUs. Launch training via `torchrun`:\n+We release **all 49** VLMs trained as part of our work, with a range of different visual representations, language\n+models, data, and scale. The exhaustive set of models (with structured descriptions) can be found in \n+[`prismatic/models/registry.py](prismatic/models/registry.py) - we will continue to update this registry as we train\n+additional models.\n \n-```bash\n-# Train VLA on Bridge V2 with the Prismatic SigLIP 224px Backbone on a Single Node (w/ 8 GPUs)\n-torchrun --standalone --nnodes 1 --nproc-per-node 8 vla-scripts/train.py \\\n-  --vla.type \"siglip-224px+mx-bridge\" \\\n-  --data_root_dir <PATH TO OXE DATA ROOT> \\\n-  --run_root_dir <PATH TO LOG/CHECKPOINT ROOT> \\\n-  --wandb_project \"<PROJECT>\" \\\n-  --wandb_entity \"<ENTITY>\"\n+We also provide a top-level API for instantiating models from the names mentioned in the various Figures of our paper,\n+as well as for generally browsing our pretrained models by description:\n+\n+```python\n+from prismatic import available_model_ids_and_names, available_model_ids, get_model_description\n+from pprint import pprint\n+\n+# List all Pretrained VLMs (by HF Hub IDs)\n+pprint(available_model_ids())\n+\n+# List all Pretrained VLMs with both HF Hub IDs AND semantically meaningful names from paper\n+pprint(available_model_ids_and_names())\n+\n+# Print and return a targeted description of a model (by name or ID) \n+#   =>> See `prismatic/models/registry.py` for explicit schema\n+description = get_model_description(\"Prism-DINOSigLIP 13B (Controlled)\")\n ```\n \n-### VLA Troubleshooting\n+Currently, our best performing models are the `Prism-DINOSigLIP` series, with especially strong performance on spatial\n+understanding and localization tasks.\n+\n+---\n+**Explicit Notes on Model Licensing & Commercial Use**: While all code in this repository is released under an MIT\n+License, our pretrained models may inherit restrictions from the _datasets_ and _underlying LMs_ we use for training. \n+\n+**[02/09/24]** Our current VLMs are all derived from Llama-2, and as such are subject to the \n+[Llama Community License](https://ai.meta.com/llama/license/), which does permit commercial use. We additionally train \n+on the LLaVa Instruct Tuning data.\n+\n+**[05/05/24]** Our new VLMs derived from Mistral and Phi-2 are subject to the original Apache and MIT Licenses attached\n+to each model.\n+\n+As we train new models, we will update this section of the README (and the LICENSE files associated with each model)\n+appropriately. If there are any questions, please file an Issue!\n \n-The following are a list of known problems and corresponding fixes:\n+## Training VLMs\n+\n+In addition to providing all pretrained VLMs trained in this work, we also provide full instructions and configurations\n+for _reproducing all results_ (down to controlling for the batch order of examples seen during training). \n+\n+#### Pretraining Datasets\n+For the [LLaVa v1.5 Instruct Dataset](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md) we use for all \n+of our models, we provide an automated download script in [`scripts/preprocess.py`](scripts/preprocess.py):\n \n ```bash\n-FileNotFoundError: Failed to construct dataset \"fractal20220817_data\", builder_kwargs \"{'data_dir': '/path/to/processed/datasets/'}\": Could not load dataset info from fractal20220817_data/0.1.0/dataset_info.json\n+# Download the `llava-v1.5-instruct` (Instruct Tuning) Image and Language Data (includes extra post-processing)\n+python scripts/preprocess.py --dataset_id \"llava-v1.5-instruct\" --root_dir <PATH-TO-DATA-ROOT>\n+\n+# (In case you also wish to download the explicit vision-language alignment data)\n+python scripts/preprocess.py --dataset_id \"llava-laion-cc-sbu-558k\" --root_dir <PATH-TO-DATA-ROOT>\n ```\n-- **Fix**: Downgrade `tensorflow-datasets` via `pip install tensorflow-datasets==4.9.3`.\n \n+As part of our work, we also train on mixtures of datasets including \n+[LVIS-Instruct-4V](https://arxiv.org/abs/2311.07574) and [LRV-Instruct](https://arxiv.org/abs/2306.14565). We provide\n+instructions and scripts for downloading these datasets in [`scripts/additional-datasets`](scripts/additional-datasets).\n+\n+We welcome any and all contributions and pull requests to add new datasets!\n+\n+#### Model Configuration & Training Script\n+\n+The entry point for training models is [`scripts/pretrain.py`](scripts/pretrain.py). We employ \n+[`draccus`](https://pypi.org/project/draccus/0.6/) to provide a modular, dataclass-based interface for specifying \n+model configurations; all 42 VLM configurations are in [`prismatic/conf/models.py`](prismatic/conf/models.py). \n+\n+We use PyTorch Fully Sharded Data Parallel (FSDP) to distribute training across GPUs, though we also provide a simpler\n+Distributed Data Parallel training implementation (for smaller LM backbones, debugging). You can run a pretraining job\n+via `torchrun`.\n+\n+As a compact example, here's how you would train a VLM derived from Vicu\u00f1a-v1.5 7B, using fused DINOv2 + SigLIP \n+representations, processing non-square images with a \"letterbox padding\" transform across 8 GPUs on a single-node: \n \n ```bash\n-AttributeError: 'DLataset' object has no attribute 'traj_map'. Did you mean: 'flat_map'?\n+# Run from the root of the repository\n+torchrun --standalone --nnodes 1 --nproc-per-node 8 scripts/pretrain.py \\\n+  --model.type \"one-stage+7b\" \\\n+  --model.model_id \"<NAME OF NEW MODEL>\" \\\n+  --model.vision_backbone_id \"dinosiglip-vit-so-384px\" \\\n+  --model.image_resize_strategy \"letterbox\" \\\n+  --model.llm_backbone_id \"vicuna-v15-7b\" \n ```\n-- **Fix**: Upgrade `dlimp` to the newest version. You may have to `--force-reinstall` like so:\n-`pip install --no-deps --force-reinstall git+https://github.com/kvablack/dlimp@\u001b[93mad72ce3a9b414db2185bc0b38461d4101a65477a\u001b[0m`\n+\n+Note that specifying `model.type` is important for identifying the _base configuration_ that you want to build on top of;\n+the full list of model types are available in our [config file](prismatic/conf/models.py), under the `model_id` key for \n+each dataclass.\n \n ---\n \n@@ -179,7 +196,7 @@ AttributeError: 'DLataset' object has no attribute 'traj_map'. Did you mean: 'fl\n High-level overview of repository/project file-tree:\n \n + `prismatic` - Package source; provides core utilities for model loading, training, data preprocessing, etc.\n-+ `vla-scripts/` - Core scripts for training, fine-tuning, and deploying VLAs.\n++ `scripts/` - Standalone scripts for preprocessing, training VLMs, and generating from pretrained models.\n + `LICENSE` - All code is made available under the MIT License; happy hacking!\n + `Makefile` - Top-level Makefile (by default, supports linting - checking & auto-fix); extend as needed.\n + `pyproject.toml` - Full project configuration details (including dependencies), as well as tool configurations.\n@@ -187,15 +204,15 @@ High-level overview of repository/project file-tree:\n \n ---\n \n-#### Citation\n+#### Citation \n \n-If you find our code or models useful in your work, please cite [our paper](https://openvla.github.io/openvla.pdf):\n+If you find our code or models useful in your work, please cite [our paper](https://arxiv.org/abs/2402.07865):\n \n ```bibtex\n-@article{kim24openvla,\n-    title={OpenVLA: An Open-Source Vision-Language-Action Model},\n-    author={{Moo Jin} Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn},\n-    journal = {arXiv preprint},\n-    year={2024}\n-} \n-```\n\\ No newline at end of file\n+@inproceedings{karamcheti2024prismatic,\n+  title = {Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models},\n+  author = {Siddharth Karamcheti and Suraj Nair and Ashwin Balakrishna and Percy Liang and Thomas Kollar and Dorsa Sadigh},\n+  booktitle = {International Conference on Machine Learning (ICML)},\n+  year = {2024},\n+}\n+```\n",
  "reason": "High Entropy",
  "stringsFound": [
    "ad72ce3a9b414db2185bc0b38461d4101a65477a"
  ]
}
----------------------------------------
Secret #6:
{
  "branch": "origin/main",
  "commit": "OpenVLA Release\n",
  "commitHash": "959c1d5848c3b396fb7e57cd32b80aa76544486e",
  "date": "2024-06-13 21:48:51",
  "diff": "@@ -3,17 +3,17 @@ requires = [\"setuptools\"]\n build-backend = \"setuptools.build_meta\"\n \n [project]\n-name = \"openvla\"\n+name = \"prismatic\"\n authors = [\n-    {name = \"Moo Jin Kim\", email=\"moojink@stanford.edu\"},\n-    {name = \"Karl Pertsch\", email=\"pertsch@berkeley.edu\"},\n     {name = \"Siddharth Karamcheti\", email=\"skaramcheti@cs.stanford.edu\"},\n+    {name = \"Suraj Nair\", email=\"suraj.nair@tri.global\"},\n+    {name = \"Ashwin Balakrishna\", email=\"ashwin.balakrishna@tri.global\"},\n ]\n-description = \"OpenVLA: Vision-Language-Action Models for Robotics\"\n-version = \"0.0.3\"\n+description = \"Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models\"\n+version = \"0.0.2\"\n readme = \"README.md\"\n requires-python = \">=3.8\"\n-keywords = [\"vision-language-actions models\", \"multimodal pretraining\", \"robot learning\"]\n+keywords = [\"vision-language models\", \"multimodal pretraining\", \"machine learning\"]\n license = {file = \"LICENSE\"}\n classifiers = [\n     \"Development Status :: 3 - Alpha\",\n@@ -31,28 +31,19 @@ classifiers = [\n ]\n dependencies = [\n     \"accelerate>=0.25.0\",\n-    \"draccus==0.8.0\",\n+    \"draccus @ git+https://github.com/dlwh/draccus\",\n     \"einops\",\n-    # \"flash_attn==2.5.5\",      # Here for documentation -- install *AFTER* editable install (follow README)\n+    # \"flash_attn>=2.5.5\",  # Here for documentation -- install *AFTER* editable install (follow README)\n     \"huggingface_hub\",\n-    \"json-numpy\",\n     \"jsonlines\",\n-    \"matplotlib\",\n-    \"peft==0.11.1\",\n-    \"protobuf\",\n     \"rich\",\n-    \"sentencepiece==0.1.99\",\n+    \"sentencepiece\",\n     \"timm==0.9.10\",\n-    \"tokenizers==0.19.1\",\n-    \"torch==2.2.0\",\n+    \"torch>=2.1.0\",\n     \"torchvision>=0.16.0\",\n     \"torchaudio\",\n-    \"transformers==4.40.1\",\n-    \"wandb\",\n-    \"tensorflow==2.15.0\",\n-    \"tensorflow_datasets==4.9.3\",\n-    \"tensorflow_graphics==2021.12.3\",\n-    \"dlimp @ git+https://github.com/kvablack/dlimp@ad72ce3a9b414db2185bc0b38461d4101a65477a\"\n+    \"transformers>=4.38.1\",\n+    \"wandb\"\n ]\n \n [project.optional-dependencies]\n@@ -63,15 +54,11 @@ dev = [\n     \"pre-commit\",\n     \"ruff>=0.2.2\",\n ]\n-sagemaker = [\n-    \"boto3\",\n-    \"sagemaker\"\n-]\n \n [project.urls]\n-homepage = \"https://github.com/openvla/openvla\"\n-repository = \"https://github.com/openvla/openvla\"\n-documentation = \"https://github.com/openvla/openvla\"\n+homepage = \"https://github.com/TRI-ML/prismatic-vlms\"\n+repository = \"https://github.com/TRI-ML/prismatic-vlms\"\n+documentation = \"https://github.com/TRI-ML/prismatic-vlms\"\n \n [tool.setuptools.packages.find]\n where = [\".\"]\n",
  "path": "pyproject.toml",
  "printDiff": "@@ -3,17 +3,17 @@ requires = [\"setuptools\"]\n build-backend = \"setuptools.build_meta\"\n \n [project]\n-name = \"openvla\"\n+name = \"prismatic\"\n authors = [\n-    {name = \"Moo Jin Kim\", email=\"moojink@stanford.edu\"},\n-    {name = \"Karl Pertsch\", email=\"pertsch@berkeley.edu\"},\n     {name = \"Siddharth Karamcheti\", email=\"skaramcheti@cs.stanford.edu\"},\n+    {name = \"Suraj Nair\", email=\"suraj.nair@tri.global\"},\n+    {name = \"Ashwin Balakrishna\", email=\"ashwin.balakrishna@tri.global\"},\n ]\n-description = \"OpenVLA: Vision-Language-Action Models for Robotics\"\n-version = \"0.0.3\"\n+description = \"Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models\"\n+version = \"0.0.2\"\n readme = \"README.md\"\n requires-python = \">=3.8\"\n-keywords = [\"vision-language-actions models\", \"multimodal pretraining\", \"robot learning\"]\n+keywords = [\"vision-language models\", \"multimodal pretraining\", \"machine learning\"]\n license = {file = \"LICENSE\"}\n classifiers = [\n     \"Development Status :: 3 - Alpha\",\n@@ -31,28 +31,19 @@ classifiers = [\n ]\n dependencies = [\n     \"accelerate>=0.25.0\",\n-    \"draccus==0.8.0\",\n+    \"draccus @ git+https://github.com/dlwh/draccus\",\n     \"einops\",\n-    # \"flash_attn==2.5.5\",      # Here for documentation -- install *AFTER* editable install (follow README)\n+    # \"flash_attn>=2.5.5\",  # Here for documentation -- install *AFTER* editable install (follow README)\n     \"huggingface_hub\",\n-    \"json-numpy\",\n     \"jsonlines\",\n-    \"matplotlib\",\n-    \"peft==0.11.1\",\n-    \"protobuf\",\n     \"rich\",\n-    \"sentencepiece==0.1.99\",\n+    \"sentencepiece\",\n     \"timm==0.9.10\",\n-    \"tokenizers==0.19.1\",\n-    \"torch==2.2.0\",\n+    \"torch>=2.1.0\",\n     \"torchvision>=0.16.0\",\n     \"torchaudio\",\n-    \"transformers==4.40.1\",\n-    \"wandb\",\n-    \"tensorflow==2.15.0\",\n-    \"tensorflow_datasets==4.9.3\",\n-    \"tensorflow_graphics==2021.12.3\",\n-    \"dlimp @ git+https://github.com/kvablack/dlimp@\u001b[93mad72ce3a9b414db2185bc0b38461d4101a65477a\u001b[0m\"\n+    \"transformers>=4.38.1\",\n+    \"wandb\"\n ]\n \n [project.optional-dependencies]\n@@ -63,15 +54,11 @@ dev = [\n     \"pre-commit\",\n     \"ruff>=0.2.2\",\n ]\n-sagemaker = [\n-    \"boto3\",\n-    \"sagemaker\"\n-]\n \n [project.urls]\n-homepage = \"https://github.com/openvla/openvla\"\n-repository = \"https://github.com/openvla/openvla\"\n-documentation = \"https://github.com/openvla/openvla\"\n+homepage = \"https://github.com/TRI-ML/prismatic-vlms\"\n+repository = \"https://github.com/TRI-ML/prismatic-vlms\"\n+documentation = \"https://github.com/TRI-ML/prismatic-vlms\"\n \n [tool.setuptools.packages.find]\n where = [\".\"]\n",
  "reason": "High Entropy",
  "stringsFound": [
    "ad72ce3a9b414db2185bc0b38461d4101a65477a"
  ]
}
----------------------------------------

Observations:
TruffleHog may produce false positives. Manually verify the findings.