TruffleHog Analysis Report for project4
============================================================
Total secrets detected: 1

Secret #1:
{
  "branch": "origin/main",
  "commit": "Initial commit\n",
  "commitHash": "40eb1c5cf31ad0e6f63c9a9ba97aa5e7a872c86b",
  "date": "2025-03-16 23:04:25",
  "diff": "@@ -0,0 +1,162 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the license found in the\n+# LICENSE file in the root directory of this source tree.\n+\n+import math\n+import numpy as np\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from vggt.layers import Mlp\n+from vggt.layers.block import Block\n+from vggt.heads.head_act import activate_pose\n+\n+\n+class CameraHead(nn.Module):\n+    \"\"\"\n+    CameraHead predicts camera parameters from token representations using iterative refinement.\n+\n+    It applies a series of transformer blocks (the \"trunk\") to dedicated camera tokens.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        dim_in: int = 2048,\n+        trunk_depth: int = 4,\n+        pose_encoding_type: str = \"absT_quaR_FoV\",\n+        num_heads: int = 16,\n+        mlp_ratio: int = 4,\n+        init_values: float = 0.01,\n+        trans_act: str = \"linear\",\n+        quat_act: str = \"linear\",\n+        fl_act: str = \"relu\",  # Field of view activations: ensures FOV values are positive.\n+    ):\n+        super().__init__()\n+\n+        if pose_encoding_type == \"absT_quaR_FoV\":\n+            self.target_dim = 9\n+        else:\n+            raise ValueError(f\"Unsupported camera encoding type: {pose_encoding_type}\")\n+\n+        self.trans_act = trans_act\n+        self.quat_act = quat_act\n+        self.fl_act = fl_act\n+        self.trunk_depth = trunk_depth\n+\n+        # Build the trunk using a sequence of transformer blocks.\n+        self.trunk = nn.Sequential(\n+            *[\n+                Block(\n+                    dim=dim_in,\n+                    num_heads=num_heads,\n+                    mlp_ratio=mlp_ratio,\n+                    init_values=init_values,\n+                )\n+                for _ in range(trunk_depth)\n+            ]\n+        )\n+\n+        # Normalizations for camera token and trunk output.\n+        self.token_norm = nn.LayerNorm(dim_in)\n+        self.trunk_norm = nn.LayerNorm(dim_in)\n+\n+        # Learnable empty camera pose token.\n+        self.empty_pose_tokens = nn.Parameter(torch.zeros(1, 1, self.target_dim))\n+        self.embed_pose = nn.Linear(self.target_dim, dim_in)\n+\n+        # Module for producing modulation parameters: shift, scale, and a gate.\n+        self.poseLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(dim_in, 3 * dim_in, bias=True))\n+\n+        # Adaptive layer normalization without affine parameters.\n+        self.adaln_norm = nn.LayerNorm(dim_in, elementwise_affine=False, eps=1e-6)\n+        self.pose_branch = Mlp(\n+            in_features=dim_in,\n+            hidden_features=dim_in // 2,\n+            out_features=self.target_dim,\n+            drop=0,\n+        )\n+\n+    def forward(self, aggregated_tokens_list: list, num_iterations: int = 4) -> list:\n+        \"\"\"\n+        Forward pass to predict camera parameters.\n+\n+        Args:\n+            aggregated_tokens_list (list): List of token tensors from the network;\n+                the last tensor is used for prediction.\n+            num_iterations (int, optional): Number of iterative refinement steps. Defaults to 4.\n+\n+        Returns:\n+            list: A list of predicted camera encodings (post-activation) from each iteration.\n+        \"\"\"\n+        # Use tokens from the last block for camera prediction.\n+        tokens = aggregated_tokens_list[-1]\n+\n+        # Extract the camera tokens\n+        pose_tokens = tokens[:, :, 0]\n+        pose_tokens = self.token_norm(pose_tokens)\n+\n+        pred_pose_enc_list = self.trunk_fn(pose_tokens, num_iterations)\n+        return pred_pose_enc_list\n+\n+    def trunk_fn(self, pose_tokens: torch.Tensor, num_iterations: int) -> list:\n+        \"\"\"\n+        Iteratively refine camera pose predictions.\n+\n+        Args:\n+            pose_tokens (torch.Tensor): Normalized camera tokens with shape [B, 1, C].\n+            num_iterations (int): Number of refinement iterations.\n+\n+        Returns:\n+            list: List of activated camera encodings from each iteration.\n+        \"\"\"\n+        B, S, C = pose_tokens.shape  # S is expected to be 1.\n+        pred_pose_enc = None\n+        pred_pose_enc_list = []\n+\n+        for _ in range(num_iterations):\n+            # Use a learned empty pose for the first iteration.\n+            if pred_pose_enc is None:\n+                module_input = self.embed_pose(self.empty_pose_tokens.expand(B, S, -1))\n+            else:\n+                # Detach the previous prediction to avoid backprop through time.\n+                pred_pose_enc = pred_pose_enc.detach()\n+                module_input = self.embed_pose(pred_pose_enc)\n+\n+            # Generate modulation parameters and split them into shift, scale, and gate components.\n+            shift_msa, scale_msa, gate_msa = self.poseLN_modulation(module_input).chunk(3, dim=-1)\n+\n+            # Adaptive layer normalization and modulation.\n+            pose_tokens_modulated = gate_msa * modulate(self.adaln_norm(pose_tokens), shift_msa, scale_msa)\n+            pose_tokens_modulated = pose_tokens_modulated + pose_tokens\n+\n+            pose_tokens_modulated = self.trunk(pose_tokens_modulated)\n+            # Compute the delta update for the pose encoding.\n+            pred_pose_enc_delta = self.pose_branch(self.trunk_norm(pose_tokens_modulated))\n+\n+            if pred_pose_enc is None:\n+                pred_pose_enc = pred_pose_enc_delta\n+            else:\n+                pred_pose_enc = pred_pose_enc + pred_pose_enc_delta\n+\n+            # Apply final activation functions for translation, quaternion, and field-of-view.\n+            activated_pose = activate_pose(\n+                pred_pose_enc,\n+                trans_act=self.trans_act,\n+                quat_act=self.quat_act,\n+                fl_act=self.fl_act,\n+            )\n+            pred_pose_enc_list.append(activated_pose)\n+\n+        return pred_pose_enc_list\n+\n+\n+def modulate(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Modulate the input tensor using scaling and shifting parameters.\n+    \"\"\"\n+    # modified from https://github.com/facebookresearch/DiT/blob/796c29e532f47bba17c5b9c5eb39b9354b8b7c64/models.py#L19\n+    return x * (1 + scale) + shift\n",
  "path": "vggt/heads/camera_head.py",
  "printDiff": "@@ -0,0 +1,162 @@\n+# Copyright (c) Meta Platforms, Inc. and affiliates.\n+# All rights reserved.\n+#\n+# This source code is licensed under the license found in the\n+# LICENSE file in the root directory of this source tree.\n+\n+import math\n+import numpy as np\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from vggt.layers import Mlp\n+from vggt.layers.block import Block\n+from vggt.heads.head_act import activate_pose\n+\n+\n+class CameraHead(nn.Module):\n+    \"\"\"\n+    CameraHead predicts camera parameters from token representations using iterative refinement.\n+\n+    It applies a series of transformer blocks (the \"trunk\") to dedicated camera tokens.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        dim_in: int = 2048,\n+        trunk_depth: int = 4,\n+        pose_encoding_type: str = \"absT_quaR_FoV\",\n+        num_heads: int = 16,\n+        mlp_ratio: int = 4,\n+        init_values: float = 0.01,\n+        trans_act: str = \"linear\",\n+        quat_act: str = \"linear\",\n+        fl_act: str = \"relu\",  # Field of view activations: ensures FOV values are positive.\n+    ):\n+        super().__init__()\n+\n+        if pose_encoding_type == \"absT_quaR_FoV\":\n+            self.target_dim = 9\n+        else:\n+            raise ValueError(f\"Unsupported camera encoding type: {pose_encoding_type}\")\n+\n+        self.trans_act = trans_act\n+        self.quat_act = quat_act\n+        self.fl_act = fl_act\n+        self.trunk_depth = trunk_depth\n+\n+        # Build the trunk using a sequence of transformer blocks.\n+        self.trunk = nn.Sequential(\n+            *[\n+                Block(\n+                    dim=dim_in,\n+                    num_heads=num_heads,\n+                    mlp_ratio=mlp_ratio,\n+                    init_values=init_values,\n+                )\n+                for _ in range(trunk_depth)\n+            ]\n+        )\n+\n+        # Normalizations for camera token and trunk output.\n+        self.token_norm = nn.LayerNorm(dim_in)\n+        self.trunk_norm = nn.LayerNorm(dim_in)\n+\n+        # Learnable empty camera pose token.\n+        self.empty_pose_tokens = nn.Parameter(torch.zeros(1, 1, self.target_dim))\n+        self.embed_pose = nn.Linear(self.target_dim, dim_in)\n+\n+        # Module for producing modulation parameters: shift, scale, and a gate.\n+        self.poseLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(dim_in, 3 * dim_in, bias=True))\n+\n+        # Adaptive layer normalization without affine parameters.\n+        self.adaln_norm = nn.LayerNorm(dim_in, elementwise_affine=False, eps=1e-6)\n+        self.pose_branch = Mlp(\n+            in_features=dim_in,\n+            hidden_features=dim_in // 2,\n+            out_features=self.target_dim,\n+            drop=0,\n+        )\n+\n+    def forward(self, aggregated_tokens_list: list, num_iterations: int = 4) -> list:\n+        \"\"\"\n+        Forward pass to predict camera parameters.\n+\n+        Args:\n+            aggregated_tokens_list (list): List of token tensors from the network;\n+                the last tensor is used for prediction.\n+            num_iterations (int, optional): Number of iterative refinement steps. Defaults to 4.\n+\n+        Returns:\n+            list: A list of predicted camera encodings (post-activation) from each iteration.\n+        \"\"\"\n+        # Use tokens from the last block for camera prediction.\n+        tokens = aggregated_tokens_list[-1]\n+\n+        # Extract the camera tokens\n+        pose_tokens = tokens[:, :, 0]\n+        pose_tokens = self.token_norm(pose_tokens)\n+\n+        pred_pose_enc_list = self.trunk_fn(pose_tokens, num_iterations)\n+        return pred_pose_enc_list\n+\n+    def trunk_fn(self, pose_tokens: torch.Tensor, num_iterations: int) -> list:\n+        \"\"\"\n+        Iteratively refine camera pose predictions.\n+\n+        Args:\n+            pose_tokens (torch.Tensor): Normalized camera tokens with shape [B, 1, C].\n+            num_iterations (int): Number of refinement iterations.\n+\n+        Returns:\n+            list: List of activated camera encodings from each iteration.\n+        \"\"\"\n+        B, S, C = pose_tokens.shape  # S is expected to be 1.\n+        pred_pose_enc = None\n+        pred_pose_enc_list = []\n+\n+        for _ in range(num_iterations):\n+            # Use a learned empty pose for the first iteration.\n+            if pred_pose_enc is None:\n+                module_input = self.embed_pose(self.empty_pose_tokens.expand(B, S, -1))\n+            else:\n+                # Detach the previous prediction to avoid backprop through time.\n+                pred_pose_enc = pred_pose_enc.detach()\n+                module_input = self.embed_pose(pred_pose_enc)\n+\n+            # Generate modulation parameters and split them into shift, scale, and gate components.\n+            shift_msa, scale_msa, gate_msa = self.poseLN_modulation(module_input).chunk(3, dim=-1)\n+\n+            # Adaptive layer normalization and modulation.\n+            pose_tokens_modulated = gate_msa * modulate(self.adaln_norm(pose_tokens), shift_msa, scale_msa)\n+            pose_tokens_modulated = pose_tokens_modulated + pose_tokens\n+\n+            pose_tokens_modulated = self.trunk(pose_tokens_modulated)\n+            # Compute the delta update for the pose encoding.\n+            pred_pose_enc_delta = self.pose_branch(self.trunk_norm(pose_tokens_modulated))\n+\n+            if pred_pose_enc is None:\n+                pred_pose_enc = pred_pose_enc_delta\n+            else:\n+                pred_pose_enc = pred_pose_enc + pred_pose_enc_delta\n+\n+            # Apply final activation functions for translation, quaternion, and field-of-view.\n+            activated_pose = activate_pose(\n+                pred_pose_enc,\n+                trans_act=self.trans_act,\n+                quat_act=self.quat_act,\n+                fl_act=self.fl_act,\n+            )\n+            pred_pose_enc_list.append(activated_pose)\n+\n+        return pred_pose_enc_list\n+\n+\n+def modulate(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Modulate the input tensor using scaling and shifting parameters.\n+    \"\"\"\n+    # modified from https://github.com/facebookresearch/DiT/blob/\u001b[93m796c29e532f47bba17c5b9c5eb39b9354b8b7c64\u001b[0m/models.py#L19\n+    return x * (1 + scale) + shift\n",
  "reason": "High Entropy",
  "stringsFound": [
    "796c29e532f47bba17c5b9c5eb39b9354b8b7c64"
  ]
}
----------------------------------------

Observations:
TruffleHog may produce false positives. Manually verify the findings.